#!/bin/bash

usage() {
  echo "The goal of podman-ollama is to make AI even more boring."
  echo
  echo "Usage:"
  echo "  podman-ollama [prompt]"
  echo "  podman-ollama [options]"
  echo "  podman-ollama [command]"
  echo
  echo "Commands:"
  echo "  serve       Start ollama server (not required)"
  echo "  create      Create a model from a Modelfile"
  echo "  chatbot     Set up chatbot UI interface"
  echo "  show        Show information for a model"
  echo "  run         Run a model, default if no command is specified"
  echo "  pull        Pull a model from a registry"
  echo "  push        Push a model to a registry"
  echo "  list        List models"
  echo "  cp          Copy a model"
  echo "  rm          Remove a model"
  echo "  help        Help about any command"
  echo "  generate    Generate structured data based on containers, pods or volumes"
  echo
  echo "Options:"
  echo "  -c, --container-manager CONMAN - Specify podman or docker, default: podman"
  echo "  -g, --gpu GPU                  - Specify a GPU: AMD, NVIDIA, GPU or CPU"
  echo "  -h, --help                     - Usage help"
  echo "  -l, --log LOGFILE              - Specify logfile to redirect to, for GPU debug"
  echo "  -m, --model MODEL              - Specify non-default model, default: mistral"
  echo "  --privileged                   - Give extended privileges to container"
  echo "  -p, --publish                  - Publish a container's port to the host"
  echo "  -r, --root                     - Run as a rootful container"
  echo "  -v, --version                  - Show version information"
  echo "  -                              - Read from stdin"
  echo
  echo "Environment Variables:"
  echo "  OLLAMA_HOST  The host:port or base URL of the Ollama server"
  echo "               (e.g. http://some_remotehost:11434)"
  echo
  echo "Configuration:"
  echo "  podman-ollama uses a simple text format to store customizations that are per"
  echo "  user in \"~/.podman-ollama/config\". Such a configuration file may look like"
  echo "  this:"
  echo
  echo "    container-manager podman"
  echo "    gpu GPU"
  echo "    model gemma:2b"
}

available() {
  command -v $1 > /dev/null
}

cleanup_impl() {
  set +e
  $SUDO $CONMAN exec $CON_PS pkill ollama
  sleep 1
  $SUDO $CONMAN rm -f $CON_PS
  $SUDO $CONMAN rm -f $CHATBOT_PS
  $SUDO $CONMAN pod rm -f $POD_PS
}

cleanup() {
  cleanup_impl > /dev/null 2>&1 &
}

check_root() {
  if [ "$EUID" -eq 0 ]; then
    if ! $ROOT; then
      echo "to run as a rootful, insecure container, use this command:"
      echo "  podman-ollama -r"
      exit 3
    fi
  elif $ROOT; then
    if available sudo; then
      SUDO="sudo"
    else
      echo "to run as a rootful, insecure container, use this command as root user:"
      echo "  podman-ollama -r"
      exit 4
    fi
  fi
}

add_dri() {
  if [ -e "/dev/dri" ]; then
    ADD="$ADD --device /dev/dri"
  fi
}

add_kfd() {
  ADD="$ADD --device /dev/kfd"
  POST="rocm"
}

gpu_setup() {
  if [ "$GPU" = "CPU" ]; then
    POST="latest"
    ADD=
  elif [ "$GPU" = "AMD" ]; then
    POST="rocm"
    ADD="$ADD --gpus=all"
    add_dri
    if [ -e "/dev/kfd" ]; then
      add_kfd
    fi
  elif [ "$GPU" = "NVIDIA" ]; then
    POST="latest"
    ADD="$ADD --gpus=all"
    ADD="$ADD --device nvidia.com/gpu=all"
    add_dri
  else
    POST="latest"
    ADD="$ADD --gpus=all"
    add_dri
    if [ -e "/dev/kfd" ]; then
      for i in /sys/bus/pci/devices/*/mem_info_vram_total; do
        # AMD GPU needs more than 512M VRAM
        if [ "$(< $i)" -gt "600000000" ]; then
          add_kfd
          break
        fi
      done
    fi

    if available nvidia-smi; then
      ADD="$ADD --device nvidia.com/gpu=all"
    fi
  fi
}

conman_run() {
  $SUDO $CONMAN run --rm $PUBLISH $PRIV --security-opt=label=disable $1 $ADD -v"$HOME":"$HOME" $VOL $URL
}

server_init() {
  check_root
  gpu_setup

  if [ "$OLLAMA_CMD" != "serve" ]; then
    trap cleanup EXIT
  fi

  if [ -n "$OLLAMA_HOST" ]; then
    ADD="$ADD -t --entrypoint /bin/bash"
    TEST_CMD="true"
    EV="-e OLLAMA_HOST=$OLLAMA_HOST"
  else
    TEST_CMD="ollama ls"
  fi

  if $CHATBOT; then
    POD_PS="$($SUDO $CONMAN pod create -p 8501:8501)"
    ADD="$ADD --pod $POD_PS"
  fi

  if [ -n "$OLLAMA_F" ]; then
    ADD="$ADD -v"$OLLAMA_F":"$OLLAMA_F""
  fi

  VOL="-vollama:/root/.ollama"
  URL="docker.io/ollama/ollama:$POST"

  if [ -n "$LOG" ]; then
    CON_PS="$(< /proc/sys/kernel/random/uuid)"
    local name="--name $CON_PS"
      conman_run "$name" > "$LOG" 2>&1 &
    while ! podman ps | grep -q $CON_PS; do
      sleep 0.01
    done
  else
    CON_PS=$(conman_run "$NAME -d")
    RET="$?"
    if [ "$RET" -ne 0 ]; then
      echo "$CON_PS"
      exit $RET
    fi
  fi

  IS_SERVER_UP="false"
  for i in {1..16}; do
    if $SUDO $CONMAN exec $CON_PS $TEST_CMD > /dev/null 2>&1; then
      IS_SERVER_UP="true"
      break
    fi

    sleep 0.01
  done

  if ! $IS_SERVER_UP; then
    echo "Ollama service failed to be responsive"
    exit 5
  fi
}

select_container_manager() {
  if available podman; then
    CONMAN="podman"
  elif available docker; then
    CONMAN="docker"
  else
    CONMAN="podman"
  fi
}

set_default_vals() {
  LLM="mistral"
  STDIN="false"
  ROOT="false"
  GENERATE="false"
  CHATBOT="false"
  GPU="GPU"
  select_container_manager
}

read_cfg_val() {
  echo "$CFG_FILE" | sed -ne "s/^$1\s//pg" 2> /dev/null
}

set_config_vals() {
  CFG_FILE="$HOME/.podman-ollama/config"
  if [ ! -e "$CFG_FILE" ]; then
    return
  fi

  CFG_FILE="$(< "$CFG_FILE")"
  local conman=$(read_cfg_val "container-manager")
  if [ -n "$conman" ]; then
    CONMAN="$conman"
  fi

  local gpu=$(read_cfg_val "gpu")
  if [ -n "$gpu" ]; then
    GPU="$gpu"
  fi

  local log=$(read_cfg_val "log")
  if [ -n "$log" ]; then
    LOG="$log"
  fi

  local model=$(read_cfg_val "model")
  if [ -n "$model" ]; then
    LLM="$model"
  fi

  local publish=$(read_cfg_val "publish")
  if [ -n "$publish" ]; then
    PUBLISH="-p $publish"
  fi

  local root=$(read_cfg_val "root")
  if [ "$root" = "true" ]; then
    ROOT="true"
  fi

  local privileged=$(read_cfg_val "privileged")
  if [ "$privileged" = "true" ]; then
    PRIV="--privileged"
  fi
}

set -e -o pipefail
shopt -s nocasematch

set_default_vals
set_config_vals

while [[ $# -gt 0 ]]; do
  case $1 in
    serve|create|show|run|pull|push|list|ls|cp|rm|help|-v|--version)
      OLLAMA_CMD="$1"
      break
      ;;
    generate)
      PODMAN_CMD="$1"
      break
      ;;
    chatbot)
      CHATBOT="true"
      break
      ;;
    -c|--container-manager)
      CONMAN="$2"
      shift 2
      ;;
    -g|--gpu)
      GPU="$2"
      shift 2
      ;;
    -h|--help)
      usage
      exit 0
      ;;
    -l|--log)
      LOG="$2"
      shift 2
      ;;
    -m|--model)
      LLM="$2"
      shift 2
      ;;
    -p|--publish)
      PUBLISH="-p $2"
      shift 2
      ;;
    --privileged)
      PRIV="--privileged"
      shift 1
      ;;
    -r|--root)
      ROOT="true"
      shift 1
      ;;
    -)
      STDIN="true"
      shift 1
      ;;
    -*|--*)
      usage
      echo "Unknown option $1"
      exit 1
      ;;
    *)
      break;
      ;;
  esac
done

ARGV=("$@")
if [ "$1" = "create" ]; then
  OLLAMA_MOD="$2"
  for (( j=0; j < $#; ++j )); do
    if [ "${ARGV[j]}" = "--file" ] || [ "${ARGV[j]}" = "-f" ]; then
      j=$((j + 1))
      OLLAMA_F="$(readlink -f ${ARGV[j]})"
      break
    fi
  done

  if [ -z "$OLLAMA_F" ] && [ -e "Modelfile" ]; then
    OLLAMA_F="$(readlink -f Modelfile)"
  fi
fi

if [ "$1" = "generate" ]; then
  for (( j=0; j < $#; ++j )); do
    if [ "${ARGV[j]}" = "--name" ]; then
      j=$((j + 1))
      NAME="--name ${ARGV[j]}"
      break
    fi
  done
fi

server_init

if [ "$1" = "serve" ]; then
  exit 0
fi

T=
if [ -t 1 ]; then
  T="-t"
fi

if $CHATBOT; then
  echo -e "\n  Local URL: http://localhost:8501"
  $SUDO $CONMAN run --rm $PRIV --security-opt=label=disable --pod $POD_PS -e MODEL_ENDPOINT="http://localhost:11434" -v"$HOME":"$HOME" quay.io/ai-lab/chatbot:latest
elif [ "$PODMAN_CMD" = "generate" ]; then
  $SUDO $CONMAN "$@"
elif [ "$OLLAMA_CMD" = "create" ]; then
  shift 2
  $SUDO $CONMAN exec $EV $T $CON_PS ollama create $OLLAMA_MOD -f "$OLLAMA_F" $*
elif [ -n "$OLLAMA_CMD" ]; then
  $SUDO $CONMAN exec $EV $T -i $CON_PS ollama $*
elif $STDIN; then
  $SUDO $CONMAN exec $EV $T $CON_PS ollama run $LLM < /dev/stdin
elif [ -n "$1" ]; then
  $SUDO $CONMAN exec $EV $T $CON_PS ollama run $LLM "$1"
else
  $SUDO $CONMAN exec $EV $T -i $CON_PS ollama run $LLM
fi

